import numpy as np
import pandas as pd
from keras.models import Sequential
from keras.layers import LSTM, Dense, Dropout
from sklearn.preprocessing import MinMaxScaler

# 백테스팅 데이터와 실제 수익률 데이터 준비
backtesting_data = np.array([17.9,9.98,-10.4,-16.97,-10.68,3.06,13.26,-17.91,8.7,14.67,-1.09,17.55,21.15,16.32,-9.6,23.32,18.7,-36.25,33.4,3.06,-51.52,21.84,31.4,-7.82,27.68,-11.58,14.14,26.69,5.36,-5.04,2.47,9.85,18.78,21.86,-14.8,26.29,-0.01,16.16,-14.06,-9.87,1.41,-9.31,5.8,-9.56,-19.98,-6.39,-16.21,-3.84,1.28,-13.72,6.43,-2.91,-3.39,9.51,25.29,-21.63,-2.68,1.83,8.94,-1.7,-18.98,33.58,-17.33,37.05,19.63,22.56,9.82,17.71,5.72,18.26,-6.87,4.7,-4.99,-3.46,1.33,16.52,15.2,4.23,-8.7,-17.73,6.99,17.32,11.64,12.7,0.29,-11.26,13.99,11.69,10.92,17.13,8,27.92,4.01,9.52,-8.8,-18.33,-2.67,2.47,18.88,9.37,2.9,-1.15,4.5,-17.17,-11.91,-4.57,-3.49,5.38,-3.25,0.51,-15.71,15.52,26.9,4.22,1.51,-2.21,-3.13,54.53,2.3,-9.85,-4.84,-6.29,22.2,-5.86,11.25,18.01,9.59,-1.94,-4.36,2.69,-2.42,13.27,-9.71,0.02,-1.07,-0.27,-2.72,-17.18,4.21,-45.04,-1.78,0.84,4.67,2.12,13.14,14.32,-5.33,-10.16,7.05,3.84,3.15,-10.74,0.07,25.66,-7.94,16.07,5.72,11.76,4.02,-42.07,33.84,-14.12,31.09,22.46,2.04,97.73,-20.72,-41.83,12.88,15.38,0.89,15.05,14.67,6.43,-1.64,-3.29,16.84,-1.31,10.26,-1.62,15.56,0.78,6.06,-0.5,-8.28,14.88,1.47,20.94,4.61,5.24,-4.7,2.81,-13.74,-3.29,-4.68,-2.51,-2.85,-8.02,6.15,-0.41,-15.96,5.84,-9.85,36.21,-11.84,4.98,2.27,-5.02,1.91,-2,-7.22,5.56,5.56,4.22,-10.8,0.93,9.29,11.8,6.65,-3.2,-1.8,-11.13,-6.29,4.06,-3.59,-6.25,-2.94,-26.42,22.48,2.82,-3.39,9.25,9.15,10.8,11.17,5.37,46.92,6.02,-14.83,-0.51,21.12,4.73,25.8,1.75,-2.32,-16.96,-1.91,1.96,-9.12,4.4,8.17,-6.45,23.6,0.88,16.84,3.49,-2.04,-2.22,13.62,-8.24,10.06,9.79,9.88,15.46,0.7,-19.93,-15.22,5.68,18.38,2.98,16.26,7.9,2.15,-9.76,-1.05,-9.93,12.17,3.73,-11.35,1.51,57.04,-2.94,0.06,-5.07,-22.01,-5.14,-25.44,2.43,-15.11,15.94,-4.39,-7.21,-45.03,-8.96,17.37,-5.35,0.88,5.91,-36.14,-0.76,26.87,5.57,3.48,2.19,14.73,4.6,-4.57,7.2,3.5,-10.02,-10.76,34.63,2.95,4.14,4.85,1.31,-2.9,-13.13,-9.68,-7.18,-10.39,6.52,18.13,8.84,4.42,23.71,3.96,2.21,-10.42,-23.91,-2.73,-3.3,2.9,10.15,-33.7,-4.77,-1.48,5.85,3.44,-6.44,8.91,2.91,-13.98,0.38,-5.05,-1.27,-25.24,6,-0.79,-1.59,11.92,0.78,-1.3,4.72,0.96,-5.03,0.34,-5.63,4.68,10.6,2.2,9.23,-1.69,8.75,-0.13,4.69,-8.38,5.61,9.12,-3.6,2.69,-3.61,5.9,10.83,-17.19,8.09,-19.33,3.12,3.46,-60.31,0.62,12.18,6.45,2.89,-7.62,5.65,-2.24,7.03,10.88,-4.73,-13.92,38.38,3.66,1.48,-1.2,38.83,8.61,1.3,-4.78,3.09,11.98,13.93,-2.19,6.03,11.16,11.76,17.7,-4.37,-3.07,-7.47,4.59,-2.54,15.77,8.01,4.32,-0.92,5.68,13.89,1.68,0.64,14.94,7.18,3.93,-0.71,9.13,13.59,6.11,-5.43,1.11,2.66,13.5,7.72,-8.63,-1.42,2.74,2.98,-3.81,1.21,4.31,-2.05,-0.38,4.78,2.39,2.1,2.78,-1.01,-17.33,26.95,7.98,3.7,8.82,-13.21,-0.28,-1.4,1.09,-0.38,-5.73,3,3.97,-3.76,1.05,-0.52,0.07,8.48,9.62,5.21,-0.47,2.6,-0.73,6.38,-0.19,0.89,-4.64,3.9,8.41,2.98,3.88,-7.76,2.68,-7.03,7.31,5.8,-5.08,4.29,-0.11,-1.35,0.35,-1.38,11.3,-3.02,-0.2,1.12,-9.95,-12.24,-7.68,6.47,8.15,0.79,8.43,1.04,-5.66,-1.34,0.81,4.69,4.89,5.35,30.45,-2.33,-5.02,4.95,-0.5,-4.17,-7.68,18.11,-1.17,-4.26,2.4,-11.56,27.07,1.93,0.53,4.75,-0.69,-19.56,-1.19,11.52,1.5,10.93,5.71,22.13,2.68,27.67,1.87,1.8,0.89,40.62,31.74,0.39,-0.53,-6.74,-0.37,1.63,8.22,-1.92,22.65,7.64,-3.35,-5.08,-15.68,-3.97,-4.21,6.32,10.28,8.38,-5.37,16.61,-2.23,8.14,8.03,17.72,5.87,5.21,5.71,2.89,11.32,29.3,8.39,4.04,-0.97,7.75,-8.22,-19.69,-11.07,-15.62,-4.55,2.63,-12.88,14.37,8.92,1.22,29.81,19.76,11.75,4.81,-0.32,-7.27,15.74,6.45,-3.3,-6.17,20.11,-19.22,-19.78,5.29,-8.04,28.55,-2.3,29.25,14.14,8.47,-1.82,13.33,4.44,-13.29,19.84,3.25,5.61,13.83,12.4,17.86,2.26,12.13,18.96,2.96,4.75,1.98,8.78,-0.31,-4.49,7.24,-25.89,-12.37,-9.02,-7.3,-5.77,-0.03,9.45,14.03,-7.96,14.55,5.05,16.09,3,-8.41,23.91,-19.2,17.82,77.53,-7.68,18.15,1.24,10.25,28.25,17.7,16.39,0.27,10.19,21.82,-21.77,41.73,-25.83,-0.13,-26.69,17.74,-13.99,7.58,-13.98,10.77,54.27,15.56,33.76,13.44,9.21,26.94,-4.38,11.33,-11.81,6.13,5.33,-3.76,-9.4,8.45,10.37,15.65,-0.57,20.42,-0.81,-28.02,9.54,11.55,2.87,9.7,-4.35,13.81,3.91,-5.06,-56.92,9.36,1.7,27.92,14.91,15.18,15.09,-7.59,2.78,-25.54,13.39,9.83,12.61,9.49,-2.81,14.23,17.25,2.02,-20.74,18.36,10.4,5.28,6.19,-14.49,27.56,15.82,-0.55,3.03,-14.46,5.13,-2.78,-8.19,7.28
])  # 예시 데이터
real_data = np.array([2.14, 5.04, -2.99, 1.85, 3.45, 4.32, 13.54, 4.09, 1.36, -10.25, 
               -2.89, -13.15, 0.59, 22.69, 0.94, -0.44, -1.80, 10.74, 5.81, 
               1.80, 2.98, 4.02, -5.04, 1.17, 1.69, -12.29, 1.06, 6.51, 
               -7.92, 5.45, 0.78, 3.46, -4.25, 2.77, -2.55, -2.36, -0.81, 
               -5.33, -6.10])  # 예시 데이터

# 데이터 전처리
scaler = MinMaxScaler(feature_range=(0, 1))
backtesting_data_scaled = scaler.fit_transform(backtesting_data.reshape(-1, 1))

# LSTM 입력 데이터 준비
def create_dataset(data, time_step=1):
    X, Y = [], []
    for i in range(len(data) - time_step - 1):
        X.append(data[i:(i + time_step), 0])
        Y.append(data[i + time_step, 0])
    return np.array(X), np.array(Y)

time_step = 10  # 예를 들어, 10일간의 데이터를 사용
X, y = create_dataset(backtesting_data_scaled, time_step)
X = X.reshape(X.shape[0], X.shape[1], 1)

# LSTM 모델 구축
model = Sequential()
model.add(LSTM(50, return_sequences=True, input_shape=(X.shape[1], 1)))
model.add(Dropout(0.2))
model.add(LSTM(50, return_sequences=False))
model.add(Dropout(0.2))
model.add(Dense(1))

model.compile(optimizer='adam', loss='mean_squared_error')

# 모델 학습
model.fit(X, y, epochs=100, batch_size=32)

# 실제 수익률 데이터를 예측
real_data_scaled = scaler.transform(real_data.reshape(-1, 1))
X_real = []

# 실제 수익률 데이터를 입력으로 사용할 시퀀스 생성
for i in range(len(real_data_scaled) - time_step):
    X_real.append(real_data_scaled[i:(i + time_step), 0])
X_real = np.array(X_real).reshape(len(X_real), time_step, 1)

# 예측
predicted = model.predict(X_real)
predicted = scaler.inverse_transform(predicted)

print(predicted)
